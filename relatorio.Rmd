---
output: 
  pdf_document:
    fig_crop: no
fontsize: 11pt
documentclass: article
geometry: margin=2cm
header-includes:
  - \usepackage[brazil, english, portuguese]{babel}
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
  - \usepackage[fixlanguage]{babelbib}
  - \usepackage{times}

  - \usepackage{graphicx}
  - \usepackage{wrapfig}
  - \usepackage{pdfpages}
  
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  
  - \usepackage{fancyhdr}
  - \usepackage{subcaption}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, message = FALSE, warning = FALSE,
  # engine.path = c(r = '', pyton = ''), 
  tidy = "formatR", tidy.opts = c(width.cutoff = 60), #tidy = "styler", 
  fig.align = "center", fig.height = 5, fig.width = 7
  )
options(
  digits = 3, 
  OutDec = ".", 
  scipen = 4, 
  xtable.comment = FALSE
  )
```

\begin{titlepage} 

\begin{center} 
{\large Universidade Estadual de Campinas}\\[0.2cm] 
{\large Instituto de Matemática, Estatística e Computação Científica}\\[0.2cm] 
{\large Departamento de Estatística - ME524}\\[4cm]

{\bf \huge Algoritmo EM para mistruras}\\[6cm]

{\large Grupo}\\[0.2cm]
{\large Victor Dalla 206493, Mariana Ferreira 183670}\\[0.2cm]
{\large Prof. Dra. Mariana Motta}\\[6cm]

{\large Campinas}\\[0.2cm]
{\large 2019}
\end{center}

\end{titlepage}


\begin{abstract}
Texto resumo.
\end{abstract}


# Introdução

Os estatísticos que trabalham com análise e modelagem de dados atualmente estão em uma posição luxuosa de conseguir estimar, prever e inferir sobre sistemas complexos de interesse, graças a métodos computacionais cada vez mais poderosos e robustos. Modelos robustos, como os modelos de mistura, constituem uma fascinante ilustração desses aspectos: enquanto dentro de uma família paramétrica, eles oferecem aproximações maleáveis em ambientes não paramétricos e, embora baseados em distribuições padrões, eles representam desafios computacionais altamente complexos \cite{marin2005bayesian}.

As distribuições de misturas compreendem um número finito ou infinito de componentes, possivelmente de diferentes tipos de distribuições, que descrevem as características dos dados. Facilitam, assim, uma descrição muito mais cuidadosa dos sistemas complexos. Por exemplo, na genética, a localização de características quantitativas em um cromossomo e a interpretação de microarranjos relacionam-se a misturas. Essa estrutura também permite que a dificuldade de um modelo de mistura seja decomposta em um conjunto de estruturas mais simples, através do uso de variáveis latentes e do algoritmo de *Expectation Maximization* (EM).


<!-- Abordagens Bayesianas à modelagem de misturas têm atraído grande interesse entre pesquisadores e praticantes. O paradigma Bayesiano permite que declarações de probabilidade sejam feitas diretamente sobre os parâmetros desconhecidos e opiniões prévias a serem incluídas na análise e modelagem do modelo.  -->


# Mistura finita

A descrição de uma mistura de distribuições é simples: qualquer combinação convexa de outras distribuições $f_i$ é uma mistura, como mostra a combinação abaixo:

$$
\quad \sum_{i=1}^{k} p_{i} f_{i}(x), \quad \sum_{i=1}^{k} p_{i}=1, \quad k>1
$$

Na maioria dos casos, as distribuições $f_i$ são de uma família paramétrica, com parâmetro desconhecido $\theta_i$, levando ao modelo de mistura paramétrica:

$$
\sum_{i=1}^{k} p_{i} f\left(x | \theta_{i}\right)
$$

Além disso, o comportamento da cauda de uma mistura é sempre descrito por um ou dois de seus componentes e que, portanto, deve refletir a escolha da família paramétrica $f\left(.| \theta_{i}\right)$. A verossimilhança $\mathbb{L}(\theta, p | x)=\prod_{i=1}^{n} \sum_{j=1}^{k} p_{j} f\left(x_{i} | \theta_{j}\right)$ de uma mistura de k distribuições tem $k^n$ termos, o que impossibilita alguma solução analítica.


<!-- Note também que a representação de misturas como combinações convexas de distribuições implica na propriedade de cálculo dos momentos:  -->

<!-- $$\mathbb{E}\left[X^{m}\right]=\sum_{i=1}^{k} p_{i} \mathbb{E}^{f_{i}}\left[X^{m}\right]$$ -->


## Algoritmo EM

Queremos encontrar o estimador de máxima verossimilhança (no contexto frequentista) ou o estimador máximo a posteriori (no contexto bayesiano), ou seja, em uma notação simplificada, queremos

$$
\hat{\theta} = \arg \max _{\theta} \mathbb{L}(\theta | x)
$$

Onde $\theta$ são os parâmetros da distribuiçao de $x$ e $\mathbb{L}$ é a verossimilhança ou a posteriori dos parâmetros nos dados observados $x$.

Apesar de existir métodos numéricos para o cômputo de máximos de função, é possível que a forma de $\mathbb{L}(\theta | x)$ sejá difícil de ser computada ou numericamente instável. O algoritmo EM é um método iterativo que pode ser capaz de otimizar $\mathbb{L}$ através da otimização de uma função (esperada ser) mais simples $Q\left(\theta | \theta^{(t-1)}, x\right)$.

### Algoritmo EM

0. Inicialização: escolha um valor para $\theta^{(0)}$

1. Passo $t$: para $t=1$ até um critério de convergência

1.1 Passo E: calcule 

$$ 
Q\left(\theta | \theta^{(t-1)}, x\right) = \mathbb{E}\left[\log \mathbb{L}(\theta | x, Z) | \theta^{(t-1)}, x \right], \quad Z \sim k\left(z | \theta^{(t-1)}, x\right)
$$

1.2 Passo M: maximize $Q$ e tome 

$$
\theta^{(t)} = \arg \max _{\theta} Q\left(\theta | \theta^{(t-1)}, x\right)
$$

O critério de convergência pode ser: pare quando $\left| \frac{\mathbb{L}(\theta^{t} | x, Z) - \mathbb{L}(\theta^{t-1} | x, Z)}{\mathbb{L}(\theta^{t-1} | x, Z)} \right| < \epsilon$.

## EM para misturas gaussianas

A probabilidade marginal da observação $x_i$ vir de um modelo de mistura k pode ser escrito como:

$$
P\left(X_{i}=x\right)=\sum_{j=1}^{k} p_{j} P\left(X_{i}=x | Z_{i}=j\right)
$$

onde $Z_{i} \in\{1, \ldots, k\}$ é a variável latente que representa o componente da mistura para $x_i$; $P\left(X_{i} | Z_{i}\right.)$ indica a mistura e $p_j$ é a probabilidade de $x_i$ pertencer ao k-ésimo componente da mistura. Como neste caso estamos interessados em misturas Gaussianas, então $X_{i} | Z_{i}=j \sim N\left(\mu_{j}, 1\right)$, podendo escrever a distribuição marginal de $x_i$ da seguinte maneira:

$$
P\left(X_{i}=x\right)=\sum_{j=1}^{k} P\left(Z_{i}=j\right) P\left(X_{i}=x | Z_{i}=j\right)=\sum_{j=1}^{k} p_{j} N\left(x ; \mu_{j}, 1\right)
$$


Então, a probabilidade conjunta (ou verossimilhança completa) das observações $X_1,...,X_n$ é dada por ($\theta=\left\{\mu_{1}, \ldots, \mu_{k}\right\}$):

$$
P\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n}\right)= L\left(\theta | X_{1}, \ldots, X_{n}\right) = \prod_{i=1}^{n} \sum_{j=1}^{k} p_{j} N\left(x_{i} ; \mu_{j}, 1\right)
$$

Aplicando o log na função de verossimilhança acima, obtemos:

$$
\ell(\theta)=\sum_{i=1}^{n} \log \left(\sum_{j=1}^{k} p_{j} N\left(x_{i} ; \mu_{j}, 1\right)\right)
$$

Porém não é possível resolver $\ell(\theta)$ analíticamente para $\mu_j$ pois, ao derivar em relação à $\mu_j$ e igualar à 0, chegamos a seguinte equação:

$$
\sum_{i=1}^{n} \frac{1}{\sum_{j=1}^{k} p_{j} N\left(x_{i} ; \mu_{j}, 1\right)} p_{j} N\left(x_{i} ; \mu_{j}, 1\right) \frac{\left(x_{i}-\mu_{j}\right)}{1}=0
$$

Mas como a variável latente $Z_i$ foi criada com o intuito de ajudar a encontrar os estimadores de máxima verossimilhança, vamos escrever a distribuição de $Z_i$ dada as observações $X_i$:

$$
P\left(Z_{i}=j | X_{i}\right)=\frac{P\left(X_{i} | Z_{i}=j\right) P\left(Z_{i}=j\right)}{P\left(X_{i}\right)}=\frac{p_{j} N\left(\mu_{j}, 1\right)}{\sum_{j=1}^{k} p_{j} N\left(\mu_{j}, 1\right)}=\gamma_{z_{i}}(j)
$$

Utilizando esta informação, podemos reescrever a equação derivada do $\ell(\theta)$ em respeito à $\mu_j$ como:

$$
\sum_{i=1}^{n} \gamma_{Z_{i}}(j) \frac{\left(x_{i}-\mu_{j}\right)}{1}=0
$$

Porém deixando a equação apenas escrita dessa maneira, $\gamma_{Z_{i}}(j)$ ainda depende de $\mu_j$. Porém o "truque" é supor que não são dependentes, obtendo assim o estimador para $\mu_j$:

$$
\hat{\mu_{j}}=\frac{\sum_{i=1}^{n} \gamma_{z_{i}}(j) x_{i}}{\sum_{i=1}^{n} \gamma_{z_{i}}(j)}=\frac{1}{N_{j}} \sum_{i=1}^{n} \gamma_{z_{i}}(j) x_{i},
$$

onde $N_{j}=\sum_{i=1}^{n} \gamma_{z_{i}}(j)$.

Se caso a variância ou o valor das probabilidades fossem desconhecidos, o estimador de máxima verossimilhança deles seriam:

$$
\hat{\sigma}_{j}^{2}=\frac{1}{N_{j}} \sum_{i=1}^{n} \gamma_{z_{i}}(j)\left(x_{i}-\mu_{j}\right)^{2}
$$

$$
\hat{p}_{j}=\frac{N_{j}}{n}
$$

Além disso, como enunciado na função  verossimilhança acima, o somatório em j indica de qual componente a observação $x_i$ pertence; no nosso caso, de qual componente da distribuição Normal. Além disso, $f_j(x_i|\mu_j)$ e $p_j$ dependem do índice j, porém não é uma informação conhecida; por este motivo utilizamos a variável latente $z_i=j$. Como em uma mitura podemos ter k componentes, então a variável $z_i$ precisa assumir k diferentes valores, um para cada componente. Logo, $z_i$ segue uma distribuição multinomial:

$$
X_{i}\left|Z_{i}=z \sim f\left(x | \theta_{z}\right), \qquad Z_{i} \sim \mathcal{M}_{k}\left(1 ; p_{1}, \ldots, p_{k}\right)\right.
$$


# Simulação

```{r lib}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(purrr)
library(metR)
```

```{r functions}
rz <- function(n, p = c(0.5, 0.5)) {
  rbinom(n, 1, p[1]) + 1
}
# P(Z = k|x)
dz <- function(k, x, mean = c(0, 0), p = c(0.5, 0.5), sd = c(1, 1)) {
  p[k] * dnorm(x, mean[k], sd[k]) / 
    purrr::map_dbl(
      x, 
      function(x) sum(p[1]*dnorm(x, mean[1], sd[1]) + p[2]*dnorm(x, mean[2], sd[2]))
    )
}

rmixnorm <- function(n, mean = c(0, 0), p = c(0.5, 0.5), sd = c(1, 1)) {
  if (length(sd) != 2 || length(p) != 2 || length(sd) != 2) stop("Tamanhos de mean, p e sd devem ser 2")
  if (sum(p) != 1) stop("Probabilidades de p não somam 1")
  
  z <- rz(n, p)
  rnorm(n, mean[z], sd[z])
}

logmixnorm_incomplete <- function(x, mean = c(0, 0), p = c(0.5, 0.5), sd = c(1, 1)) {
  if (length(sd) != 2 || length(p) != 2 || length(sd) != 2) stop("Tamanhos de mean, p e sd devem ser 2")
  if (sum(as_vector(p)) != 1) stop("Probabilidades de p não somam 1")
  
  sum(log(p[1]*dnorm(x, mean[1], sd[1]) + p[2]*dnorm(x, mean[2], sd[2])))
}

logmixnorm_complete <- function(x, z, mean = c(0, 0), p = c(0.5, 0.5), sd = c(1, 1)) {
  if (length(sd) != 2 || length(p) != 2 || length(sd) != 2) stop("Tamanhos de mean, p e sd devem ser 2")
  if (sum(p) != 1) stop("Probabilidades de p não somam 1")
  
  sum(purrr::map2_dbl(
    x, z,
    function(x, z) log(p[z]) + dnorm(x, mean[z], sd[z], log = TRUE)
    ))
  
  # sum <- 0
  # for (i in 1:n) {
  #   sum <- sum + log(p[z[i]]) + dnorm(x[i], mean[z[i]], sd[z[i]], log = TRUE)
  # }
  # sum
}
```

```{r em}
run_EM <- function(mean1, mean2, x, p, sd = c(1, 1), M = 100, converg = 1e-5) {
  result <- tibble(it = numeric(M), log_lik = numeric(M), mean1 = numeric(M), mean2 = numeric(M))
  # valores iniciais
  mean <- c(mean1, mean2)
  log_lik <- logmixnorm_incomplete(x, mean, p, sd)
  delta <- abs(log_lik)
  for (i in 1:M) {
    if (delta < converg) {
      message(paste("converged at iteration", i))
      break
    }
    
    # guarda resultado
    result[i, ] <- c(i, log_lik, mean[1], mean[2])
    
    # update mean
    expct <- list(dz(rep(1, length(x)), x, mean, p), dz(rep(2, length(x)), x, mean, p))
    mean <- c(sum(x * expct[[1]]), sum(x * expct[[2]])) / 
      c(sum(expct[[1]]), sum(expct[[2]]))
    
    # update critério de parada
    delta <- abs((log_lik - logmixnorm_incomplete(x, mean, p, sd)) / log_lik)
    log_lik <- logmixnorm_incomplete(x, mean, p, sd)
  }
  
  if (i == M) warning(paste("log_lik doesn't converged after ", M, "steps with converg="), converg)
  filter(result, it > 0)
}
```

```{r data, echo=FALSE}
n <- 500
mean <- c(0, 2.5)
p <- c(0.7, 0.3)
sd <- c(1, 1)
set.seed(42)
x <- rmixnorm(n, mean, p)

grid_space <- 0.2
grid <- expand.grid(
  mu1 = seq(from = -3, to = 7, by = grid_space), 
  mu2 = seq(from = -3, to = 7, by = grid_space)
  )
grid$log_lik <- map2_dbl(grid$mu1, grid$mu2, function(mu1, mu2) logmixnorm_incomplete(x, c(mu1, mu2), p, sd))
```


Foi gerada uma amostra de tamanho $n=500$ da mistura gaussiana $p N\left(\mu_{1}, 1\right)+(1-p) \mathcal{N}\left(\mu_{2}, 1\right)$, com $\left(\mu_{1}, \mu_{2}, p\right)=(0,2.5,0.7)$. A Figura \ref{fig:logvero} apresenta uma curva de nível log-verossimilhança no dados. Note que essa função tem duas modas: uma representa a máxima verossimilhança (próxima de $(\mu_1=0, \mu_2=2.5)$) e a outra é espúria (próxima de $(\mu_1=1, \mu_2=3)$). Isso representa uma dificuldade para qualquer algoritmo de otimização de função.

```{r graf1, fig.pos = "H", fig.cap="Gráfico da curva de nível log-verossimilhança\\label{fig:logvero}"}
ggplot(grid, aes(x = mu1, y = mu2, z = log_lik)) + 
  geom_raster(aes(fill = log_lik)) + 
  geom_contour2(lineend = "round", linejoin = "round", binwidth = 100, col = "white") + 
  geom_text_contour(stroke = 0.1) + 
  scale_fill_distiller(palette = "RdYlBu") + 
  labs(x = expression(mu[1]), y = expression(mu[2]), fill = expression(paste("l(", c(mu, p), "|", x, ")"))) + 
  theme_minimal()
```

Com o objetivo de avaliar a performance do algoritmo EM, cinco execuções foram feitas com pontos inciais $(\mu_1^{(0)}, \mu_1^{(0)}) = (5, 3), (3, 5), (-2, -2)$. A Figura \ref{fig:caminhoem} apresenta os caminhos tomados pelo algoritmo EM para esses valores iniciais. Ela ilustra três situações gerais:

1. Pontos iniciais próximos de $(\mu_1=0, \mu_2=2.5)$ levam o EM para o máximo correto.
2. Pontos iniciais próximos de $(\mu_1=1, \mu_2=3)$ levam o EM para o máximo espúrio.
3. Pontos iniciais na reta $\mu_1=\mu_2$ levam o EM para o ponto de cela ao redor de $(1.71, 1.71)$.

Isso significa que o ponto incial do EM não deve ser escolhido de maneira arbitrária: o ponto inicial deve estar próximo da estimativa procurada. Portanto, se não há indícios de que $\mu_1=\mu_2$, o ponto incial não deveria pertencer à reta $\mu_1=\mu_2$.

```{r, fig.pos = "H", fig.cap="Gráfico da curva de nível log-verossimilhança com os caminhos do EM para diversos valores iniciais de $(\mu_1, \mu_2)$\\label{fig:caminhoem}"}
runs <- list(run_EM(5, 3, x, p, sd), run_EM(3, 5, x, p, sd), run_EM(-2, -2, x, p, sd))

ggplot(grid, aes(x = mu1, y = mu2, z = log_lik)) + 
  geom_raster(aes(fill = log_lik)) + 
  geom_contour2(lineend = "round", linejoin = "round", binwidth = 100, col = "white") + 
  geom_text_contour(stroke = 0.1) + 
  scale_fill_distiller(palette = "RdYlBu") + 
  
  geom_path(data = runs[[1]], aes(mean1, mean2), col = "aquamarine3") + 
  geom_point(data = runs[[1]], aes(mean1, mean2), col = "aquamarine4") + 
  
  geom_path(data = runs[[2]], aes(mean1, mean2), col = "gold2") + 
  geom_point(data = runs[[2]], aes(mean1, mean2), col = "gold3") + 
  
  geom_path(data = runs[[3]], aes(mean1, mean2), col = "green2") + 
  geom_point(data = runs[[3]], aes(mean1, mean2), col = "green3") + 
  
  labs(x = expression(mu[1]), y = expression(mu[2]), fill = expression(paste("l(", c(mu, p), "|", x, ")"))) + 
    theme_minimal()
```


# Conclusão


\bibliographystyle{plain}
\bibliography{bibliography}

