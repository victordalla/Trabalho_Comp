---
output: 
  pdf_document:
    fig_crop: no
fontsize: 11pt
documentclass: article
bibliography: bibliography.bib
geometry: margin=2cm
header-includes:
  - \usepackage[brazil, english, portuguese]{babel}
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
  - \usepackage[fixlanguage]{babelbib}
  - \usepackage{times}

  - \usepackage{graphicx}
  - \usepackage{wrapfig}
  - \usepackage{pdfpages}
  
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  
  - \usepackage{fancyhdr}
  - \usepackage{subcaption}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, message = FALSE, warning = FALSE,
  # engine.path = c(r = '', pyton = ''), 
  #tidy = "formatR", 
  tidy.opts = c(width.cutoff = 60), #tidy = "styler", 
  fig.align = "center", fig.height = 5, fig.width = 7
  )
options(
  digits = 3, 
  OutDec = ".", 
  scipen = 4, 
  xtable.comment = FALSE
  )
```

\begin{titlepage} 

\begin{center} 
{\large Universidade Estadual de Campinas}\\[0.2cm] 
{\large Instituto de Matemática, Estatística e Computação Científica}\\[0.2cm] 
{\large Departamento de Estatística - ME524}\\[4cm]

{\bf \huge Algoritmo EM para mistruras}\\[6cm]

{\large Grupo}\\[0.2cm]
{\large Victor Dalla 206493, Mariana Ferreira 183670}\\[0.2cm]
{\large Prof. Dra. Mariana Motta}\\[6cm]

{\large Campinas}\\[0.2cm]
{\large 2019}
\end{center}

\end{titlepage}


\begin{abstract}
O uso de misturas se faz cada mais importante devido à sua capacidade de modelar sistemas complexos e flexibilidade. No entanto, sua formulação impede a estimação de parâmetros por métodos como o de máximo de verossimilhança, num contexto frequentista, ou \textit{maximum a posterior} (MAP), no caso bayesiano. Assim, uma das formas de se cortornar esse problema é através da formulação de variáveis latentes e o algoritmo EM. Essas variáveis latentes definem de quais componentes da mistura a observação foi gerada. O algoritmo EM, então, maximiza a verossimilhança da mistura através de um processo iterativo que, a cada passo, maximiza a verossimilhança completa utilizando o valor esperado da variável latente. No entanto, é possível que esse processo termine em um máximo local ou ponto de cela, visto que requer um ponto incial para a incialização.

Com o intuito de verificar esses problemas relacionado ao EM, foi realizada uma simulação de uma mistura gaussiana de duas componentes. Então, foram realizadas diversas execuções do EM com pontos iniciais distintos. Os resultados indicam que (i) o ponto incial deve estar na zona de influência do ponto de máximo global, caso contrário, um ponto de máximo local será encontrado, e (ii) para o caso da mistura gaussiana de duas componentes, pontos inciais na reta identidade terminam num ponto de cela, também na reta identidade. Isso indica que a decisão do ponto inicial é crucial e pode ser tomada com base numa crença do estatístico -- com base nas informações a priori e análise descritiva dos dados obsevados -- do valor do parâmetro de interesse.
\end{abstract}


# Introdução

Os estatísticos que trabalham com análise e modelagem de dados atualmente estão em uma posição luxuosa de conseguir estimar, prever e inferir sobre sistemas complexos de interesse, graças a métodos computacionais cada vez mais poderosos e robustos. Modelos robustos, como os modelos de mistura, constituem uma fascinante ilustração desses aspectos: enquanto dentro de uma família paramétrica, eles oferecem aproximações maleáveis em ambientes não paramétricos e, embora baseados em distribuições padrões, eles representam desafios computacionais altamente complexos [@marin2005bayesian].

As distribuições de misturas compreendem um número finito ou infinito de componentes, possivelmente de diferentes tipos de distribuições, que descrevem as características dos dados. Facilitam, assim, uma descrição muito mais cuidadosa dos sistemas complexos. Por exemplo, na genética, a localização de características quantitativas em um cromossomo e a interpretação de microarranjos relacionam-se a misturas. Essa estrutura também permite que a dificuldade de um modelo de mistura seja decomposta em um conjunto de estruturas mais simples, através do uso de variáveis latentes e do algoritmo de *Expectation Maximization* (EM).


<!-- Abordagens Bayesianas à modelagem de misturas têm atraído grande interesse entre pesquisadores e praticantes. O paradigma Bayesiano permite que declarações de probabilidade sejam feitas diretamente sobre os parâmetros desconhecidos e opiniões prévias a serem incluídas na análise e modelagem do modelo.  -->


# Mistura finita

A descrição de uma mistura de distribuições é simples: uma combinação convexa de distribuições $f_i$. Em termos matemáticos:

$$
\quad \sum_{i=1}^{k} p_{i} f_{i}(x), \quad \sum_{i=1}^{k} p_{i}=1, \quad k>1
$$

Na maioria dos casos, as distribuições $f_i$ são de uma família paramétrica, com parâmetro desconhecido $\theta_i$, levando ao modelo de mistura paramétrica:

$$
\sum_{i=1}^{k} p_{i} f\left(x | \theta_{i}\right)
$$

Além disso, o comportamento da cauda de uma mistura é sempre descrito por um ou dois de seus componentes e que, portanto, deve refletir a escolha da família paramétrica $f\left(.| \theta_{i}\right)$. A verossimilhança $L(\theta, p | x)=\prod_{i=1}^{n} \sum_{j=1}^{k} p_{j} f\left(x_{i} | \theta_{j}\right)$ de uma mistura de k distribuições tem $k^n$ termos, o que impossibilita alguma solução analítica. A verossimilhança pode ser entendida como uma posteriori em um contexto Bayesiano. No entanto, normalmente a família escolhida para $f$ possui prioris pseudo-conjugadas. Por isso, virtualmente qualquer resultado analisado aqui é válido para o caso Bayesiano.


## Variáveis latentes

Variáveis latente são variáveis que não são diretamente observadas, mas estão diretamente relacionadas à geração dos dados e podem ser inferidas. Variáveis latentes geralmente descrevem alguma característica subjacente do sistema sendo investigado.

Para misturas, a variável latente é usada para indicar de qual distribuição a observação se origina e é explorada como um instrumento para facilitar as estimações. É possível assossiar uma variável aleatória $X$ de uma mistura de $k$ distribuições à uma outra variável aleatória $Z_i$ tal que:

$$ X_{i}\left|Z_{i} \sim \mathcal{N}\left(x | \theta_{Z_i}\right), \qquad Z_{i} \sim \mathcal{M}_{k}\left(1 ; p_{1}, \ldots, p_{k}\right)\right.$$

onde $\mathcal{M}_{k}(1 ; p_{1}, \ldots, p_{k})$ denota uma distribuição multinomial com $k$ modalidades e uma única observação. Essa variável auxiliar identifica qual compenente a observação $x_i$ pertence.

A probabilidade marginal da observação $x_i$ vir de um modelo de mistura k pode ser escrito como:

$$P\left(X_{i}=x\right)=\sum_{j=1}^{k} P\left(Z_{i}=j\right) P\left(X_{i}=x | Z_{i}=j\right)$$

onde $Z_{i} \in\{1, \ldots, k\}$ é a variável latente que determina de qual componente da mistura $x_i$ foi amostrado e $P\left(Z_{i}=j\right) = p_j$ é a probabilidade de $x_i$ pertencer ao k-ésimo componente da mistura.


# Expectation Maximization

Queremos encontrar o estimador de máxima verossimilhança (no contexto frequentista) ou o estimador máximo a posteriori (no contexto bayesiano), ou seja, em uma notação simplificada, queremos

$$
\hat{\theta} = \arg \max _{\theta} L(\theta | x)
$$

Onde $\theta$ são os parâmetros da distribuiçao de $x$ e $L$ é a verossimilhança.

Apesar de existir métodos numéricos para o cômputo de máximos de função, é possível que a forma de $L(\theta | x)$ seja difícil de ser computada ou numericamente instável. O algoritmo EM é um método iterativo que pode ser capaz de otimizar $L$ através da otimização de uma função (esperada ser) mais simples $Q\left(\theta | \theta^{(t-1)}, x\right)$.

### Algoritmo EM

0. Inicialização: escolha um valor para $\theta^{(0)}$

1. Passo $t$: para $t=1$ até um critério de convergência

1.1 Passo E: calcule 

$$ 
Q\left(\theta | \theta^{(t-1)}, x\right) = \mathbb{E}\left[\log L(\theta | x, Z) | \theta^{(t-1)}, x \right], \quad Z \sim k\left(z | \theta^{(t-1)}, x\right)
$$

1.2 Passo M: maximize $Q$ e tome 

$$
\theta^{(t)} = \arg \max _{\theta} Q\left(\theta | \theta^{(t-1)}, x\right)
$$

O critério de convergência pode ser: pare quando $\left| \frac{L(\theta^{t} | x, Z) - L(\theta^{t-1} | x, Z)}{L(\theta^{t-1} | x, Z)} \right| < \epsilon$.


## EM para misturas gaussianas

Se $X$ é v.a. de uma mistura gaussiana, então

$$
P\left(X=x\right) = \sum_{j=1}^{k} p_{j} \mathcal{N}\left(x ; \mu_{j}, 1\right)
$$

E a probabilidade conjunta (ou verossimilhança completa) das observações $X_1,...,X_n$, supondo independência, é dada por:

$$
P\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n}\right)= L\left(\theta | X_{1}, \ldots, X_{n}\right) = \prod_{i=1}^{n} \sum_{j=1}^{k} p_{j} \mathcal{N}\left(x_{i} ; \mu_{j}, 1\right)
$$

Tomando o logritmo da função de verossimilhança acima, obtemos:

$$
\ell(\theta)=\sum_{i=1}^{n} \log \left(\sum_{j=1}^{k} p_{j} \mathcal{N}\left(x_{i} ; \mu_{j}, 1\right)\right)
$$

Porém não é possível resolver $\ell(\theta)$ analiticamente para $\mu_j$: igualando o gradiente de $\ell(\theta)$ a zero, obtém-se as seguintes equações:

$$
\sum_{i=1}^{n} \frac{1}{\sum_{j=1}^{k} p_{j} \mathcal{N}\left(x_{i} ; \mu_{j}, 1\right)} p_{j} \mathcal{N}\left(x_{i} ; \mu_{j}, 1\right) \frac{\left(x_{i}-\mu_{j}\right)}{\sigma_{j}^2}=0, \qquad j = 1, ..., k
$$

Portanto, faz-se necessário o uso do algoritmo EM. Para tanto, é preciso determinar a distribuição de $Z \sim k\left(z | \mu_1, ..., \mu_k, x\right)$. Condicionando em $X_i$ e usando a independência dos dados, encontra-se

$$
P\left(Z_{i}=j | X_{i}\right)=\frac{P\left(X_{i} | Z_{i}=j\right) P\left(Z_{i}=j\right)}{P\left(X_{i}\right)}=\frac{p_{j} \mathcal{N}\left(\mu_{j}, 1\right)}{\sum_{j=1}^{k} p_{j} \mathcal{N}\left(\mu_{j}, 1\right)}=\gamma_{Z_{i}}(j)
$$

Utilizando esta informação, podemos reescrever a equação do gradiente como

$$
\sum_{i=1}^{n} \gamma_{Z_{i}}(j) \frac{\left(x_{i}-\mu_{j}\right)}{\sigma_{j}^2}=0
$$

Resolvendo para $\mu_j$, obtém-se um estimador $\hat{\mu_{j}}$ para $\mu_j$:

$$
\hat{\mu}_{j}=\frac{\sum_{i=1}^{n} x_{i} \gamma_{Z_{i}}(j)}{\sum_{i=1}^{n} \gamma_{Z_{i}}(j)}=\frac{1}{N_{j}} \sum_{i=1}^{n} x_{i} \gamma_{Z_{i}}(j),
$$

onde $N_{j}=\sum_{i=1}^{n} \gamma_{z_{i}}(j)$.

Caso as variâncias e os valores das probabilidades fossem desconhecidos, os estimadores de máxima verossimilhança são facilmente calculados no modelo de variáveis latentes e poderiam ser incorporados no EM:

$$
\hat{\sigma}_{j}^{2}=\frac{1}{N_{j}} \sum_{i=1}^{n} \gamma_{Z_{i}}(j)\left(x_{i}-\mu_{j}\right)^{2}, \qquad \hat{p}_{j}=\frac{N_{j}}{n}
$$

Note que os estimadores acima dependem de $N_j$ e $\gamma_{Z_{i}}(j)$, que por sua vez são funções da variável latente não observada $Z$. Para contornar esse problema, é razoável tomar seu valor esperado. No entanto, esse valor depende de $\mu_1, ..., \mu_k$, que também são desconhecidos. O algoritmo EM contorna esse problema através de um processo iterativo, descrito antriormente e desenvolvido aqui de maneira intuitiva. $\mathbb{E}(Z_1), ..., \mathbb{E}(Z_n)$ são calculados por valores iniciais de $\mu_1, ..., \mu_k$. De posse desses valores, é possível calcular $\hat{\mu}_1, ..., \hat{\mu}_k$ e essas estimativas são usados para obter aproximações melhores de $\mathbb{E}(Z_1), ..., \mathbb{E}(Z_n)$, que por sua vez serão usados para melhorar as estimativas $\hat{\mu}_1, ..., \hat{\mu}_k$, num processo iterativo.

# Simulação

Os pacotes utilizados neste trabalho foram: `dplyr` (para manipulação de \textit{data frames}), `ggplot2` (para criação de gráficos), e `metR` (para o desenho de curvas de níveis).

```{r lib}
library(dplyr)
library(ggplot2)
library(metR)
```

As funções definidas abaixo são utilizadas extensivamente por toda a análise. A função `rz` gera valores aleatório de $Z = 1 \text{, com probabilidade } p \text{ e } Z=2 \text{ com probabilidade } 1-p$; `dz` calcula $P(Z=k | X)$; `rmixnorm` gera valores aleatórios de uma mistura gaussiana de duas componentes e `logmixnorm_incomplete` calcula $L(\mu_1, \mu_2 | x)$.

```{r functions, echo=TRUE}
rz <- function(n, p) {
  rbinom(n, 1, p[1]) + 1
}

dz <- function(k, x, mean, p, sd) {
  total <- numeric(length(x))
  for (i in 1:length(x)) {
    total[i] <- p[1]*dnorm(x[i], mean[1], sd[1]) + p[2]*dnorm(x[i], mean[2], sd[2])
  }
  p[k] * dnorm(x, mean[k], sd[k]) / total
}

rmixnorm <- function(n, mean, p, sd) {
  if (length(sd) != 2 || length(p) != 2 || length(sd) != 2) stop("Tamanhos de mean, p e sd devem ser 2")
  if (sum(p) != 1) stop("Probabilidades de p não somam 1")
  
  z <- rz(n, p)
  rnorm(n, mean[z], sd[z])
}

logmixnorm_incomplete <- function(x, mean, p, sd) {
  if (length(sd) != 2 || length(p) != 2 || length(sd) != 2) stop("Tamanhos de mean, p e sd devem ser 2")
  if (sum(p) != 1) stop("Probabilidades de p não somam 1")
  
  sum(log(p[1]*dnorm(x, mean[1], sd[1]) + p[2]*dnorm(x, mean[2], sd[2])))
}
```


Considere uma amostra $x$ de tamanho $n=500$ da mistura gaussiana $p \mathcal{N}\left(\mu_{1}, 1\right)+(1-p) \mathcal{N}\left(\mu_{2}, 1\right)$, com $\left(\mu_{1}, \mu_{2}, p\right)=(0,2.5,0.7)$, como gerada no bloco de código abaixo. A Figura \ref{fig:logvero} apresenta uma curva de nível da log-verossimilhança no dados. Note que essa função tem duas modas: uma representa a máxima verossimilhança (próxima de $(\mu_1=0, \mu_2=2.5)$) e a outra é espúria (próxima de $(\mu_1=1, \mu_2=3)$). Isso representa uma dificuldade para qualquer algoritmo de otimização de função.

```{r data, echo=TRUE}
n <- 500
mean <- c(0, 2.5)
p <- c(0.7, 0.3)
sd <- c(1, 1)
set.seed(42)
x <- rmixnorm(n, mean, p, sd)
```


```{r logvero, fig.pos = "H", fig.cap="Gráfico da curva de nível log-verossimilhança. O ponto é (0, 2.5), que é próximo da máxima verossimilhança.\\label{fig:logvero}"}
grid_space <- 0.2
grid <- expand.grid(
  mu1 = seq(from = -3, to = 7, by = grid_space), 
  mu2 = seq(from = -3, to = 7, by = grid_space)
  )
grid$log_lik <- numeric(nrow(grid))
for (i in 1:nrow(grid)) {
  grid$log_lik[i] <- logmixnorm_incomplete(x, c(grid$mu1[i], grid$mu2[i]), p, sd)
}

ggplot(grid, aes(x = mu1, y = mu2, z = log_lik)) + 
  geom_raster(aes(fill = log_lik)) + 
  geom_contour2(lineend = "round", linejoin = "round", binwidth = 100, col = "white") + 
  geom_text_contour(stroke = 0.1) + 
  scale_fill_distiller(palette = "RdYlBu") + 
  annotate("point", x = 2.5, y = 0, col = "aquamarine3") + 
  coord_flip() + 
  labs(x = expression(mu[2]), y = expression(mu[1]), fill = expression(paste("l(", c(mu, p), "|", x, ")"))) + 
  theme_minimal()
```

O bloco de código abaixo define uma função que performa o algoritmo EM para uma mistura gaussiana de dois componentes. Ela requer valores iniciais `mean1` e `mean2` e termina a execução quando o critério de parada $\left| \frac{\mathbb{L}(\theta^{t} | x, Z) - \mathbb{L}(\theta^{t-1} | x, Z)}{\mathbb{L}(\theta^{t-1} | x, Z)} \right| < \text{converg}, \text{converg}=10^{-5}$ é satisfeito ou o número máximo de iterações $M=100$ é excedido.

```{r em, echo=TRUE}
run_EM <- function(mean1, mean2, x, p, sd, M = 100, converg = 1e-5) {
  result <- tibble(it = numeric(M), log_lik = numeric(M), mean1 = numeric(M), mean2 = numeric(M))
  # valores iniciais
  mean <- c(mean1, mean2)
  log_lik <- logmixnorm_incomplete(x, mean, p, sd)
  if (is.infinite(log_lik)) stop(paste("Log verossimilhança em (", mean1, ", ", mean2, ") não é finita"))
  if (is.na(log_lik)) stop(paste("Log verossimilhança em (", mean1, ", ", mean2, ") retornou NA"))
  delta <- abs(log_lik)
  for (i in 1:M) {
    if (delta < converg) {
      message(paste("Log verossimilhança convergiu na iteração", i))
      break
    }
    
    result[i, ] <- c(i, log_lik, mean[1], mean[2])
    
    expct <- list(dz(rep(1, length(x)), x, mean, p, sd), dz(rep(2, length(x)), x, mean, p, sd))
    mean <- c(sum(x * expct[[1]]), sum(x * expct[[2]])) / c(sum(expct[[1]]), sum(expct[[2]]))
    
    delta <- abs((log_lik - logmixnorm_incomplete(x, mean, p, sd)) / log_lik)
    log_lik <- logmixnorm_incomplete(x, mean, p, sd)
  }
  
  if (i == M) warning(paste("Log verossimilhança não convergiu após número máximo M=", M, "de iterações com converg="), converg)
  filter(result, it > 0)
}
```

Com o objetivo de avaliar a performance do algoritmo EM, três execuções foram feitas com pontos inciais $(\mu_1^{(0)}, \mu_1^{(0)}) = (5, 3), (3, 5), (-2, -2)$. A Figura \ref{fig:caminhoem} apresenta os caminhos tomados pelo algoritmo EM para esses valores iniciais. Ela ilustra três situações gerais:

1. Pontos iniciais próximos de $(\mu_1=0, \mu_2=2.5)$ levam o EM para o máximo correto.
2. Pontos iniciais próximos de $(\mu_1=1, \mu_2=3)$ levam o EM para o máximo espúrio.
3. Pontos iniciais na reta $\mu_1=\mu_2$ levam o EM para o ponto de cela ao redor de $(1.71, 1.71)$.

Isso significa que o ponto incial do EM não deve ser escolhido de maneira arbitrária: o ponto inicial deve estar próximo da estimativa procurada. Portanto, se não há indícios de que $\mu_1=\mu_2$, o ponto incial não deveria pertencer à reta $\mu_1=\mu_2$.

```{r, fig.pos = "H", fig.cap="Gráfico da curva de nível log-verossimilhança com os caminhos do EM para diversos valores iniciais de $(\\mu_1, \\mu_2)$\\label{fig:caminhoem}"}
runs <- list(run_EM(5, 3, x, p, sd), run_EM(3, 5, x, p, sd), run_EM(-2, -2, x, p, sd))

ggplot(grid, aes(x = mu1, y = mu2, z = log_lik)) + 
  geom_raster(aes(fill = log_lik)) + 
  geom_contour2(lineend = "round", linejoin = "round", binwidth = 100, col = "white") + 
  geom_text_contour(stroke = 0.1) + 
  scale_fill_distiller(palette = "RdYlBu") + 
  
  geom_path(data = runs[[1]], aes(mean1, mean2), col = "aquamarine3") + 
  geom_point(data = runs[[1]], aes(mean1, mean2), col = "aquamarine4") + 
  
  geom_path(data = runs[[2]], aes(mean1, mean2), col = "gold2") + 
  geom_point(data = runs[[2]], aes(mean1, mean2), col = "gold3") + 
  
  geom_path(data = runs[[3]], aes(mean1, mean2), col = "green2") + 
  geom_point(data = runs[[3]], aes(mean1, mean2), col = "green3") + 
  
  coord_flip() + 
  labs(x = expression(mu[1]), y = expression(mu[2]), fill = expression(paste("l(", c(mu, p), "|", x, ")"))) + 
    theme_minimal()
```


# Conclusão

Os resultados da simulação mostram que a utilização do algoritmo EM para misturas é sensível ao valor inicial, pois a estimativa do algoritmo pode resultar em um máximo local ou ponto de cela a depender do ponto incial. Logo, é importante iniciar o algoritmo com valores próximos do que se espera ser o máximo global, utilizando para isso, por exemplo, informações à priori e análises sobre as observções. Uma sugestão, portanto, é executar o algoritmo para uma variedade de valores iniciais e analisar a verossimilhança de cada resultado.

Além disso, para misturaa gaussianaa de duas componentes, conjectura-se que a reta identidade leva a um ponto de cela da verossimilhança, isto é, pontos inciais na reta identidade resultam em uma estimativa no ponto de cela, também pertencente à reta identidade. Apesar disso, como exposto acima, o ponto incial deveria refletir informações disponíveis para o estatístico, o que significa que isso, em teoria, não seria um problema, já que a modelagem por mistura normalamente não se faz adequada para dados unimodais.

# Referências

