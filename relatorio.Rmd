---
output: 
  pdf_document:
    fig_crop: no
bibliography: bibliography.bib
fontsize: 11pt
documentclass: article
geometry: margin=2cm
header-includes:
  - \usepackage[brazil, english, portuguese]{babel}
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
  - \usepackage[fixlanguage]{babelbib}
  - \usepackage{times}

  - \usepackage{graphicx}
  - \usepackage{wrapfig}
  - \usepackage{pdfpages}
  
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  
  - \usepackage{fancyhdr}
  - \usepackage{subcaption}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE,
  warning = FALSE,
  tidy.opts = list(width.cutoff = 60),
  tidy = TRUE
  )
options(
  OutDec = ".", 
  knitr.table.format = "latex", 
  xtable.comment = FALSE
  )
```

```{r lib}
library(dplyr)
library(ggplot2)
library(purrr)
```

\begin{titlepage} 

\begin{center} 
{\large Universidade Estadual de Campinas}\\[0.2cm] 
{\large Instituto de Matemática, Estatística e Computação Científica}\\[0.2cm] 
{\large Departamento de Estatística - ME524}\\[4cm]

{\bf \huge Algoritmo EM para mistruras}\\[6cm]

{\large Grupo}\\[0.2cm]
{\large Victor Dalla 206493, Mariana Ferreira 183670}\\[0.2cm]
{\large Prof. Dra. Mariana Motta}\\[6cm]

{\large Campinas}\\[0.2cm]
{\large 2019}
\end{center}

\end{titlepage}


\begin{abstract}
Texto resumo.
\end{abstract}


# Introdução

- comentar misturas e abordagem bayesiana
- ferramentas computacionais

Os estatísticos que trabalham com análise e modelagem de dados atualmente estão em uma posição luxuosa de conseguir estimar, prever e inferir sobre sistemas complexos de interesse, graças a métodos computacionais cada vez mais poderosos e robustos. Modelos robustos, como os modelos de mistura, constituem uma fascinante ilustração desses aspectos: enquanto dentro de uma família paramétrica, eles oferecem aproximações maleáveis em ambientes não paramétricos e, embora baseados em distribuições padrões, eles representam desafios computacionais altamente complexos.\newline
As distribuições de misturas compreendem um número finito ou infinito de componentes, possivelmente de diferentes tipos de distribuições, que descrevem as características dos dados. Facilitam, assim, uma descrição muito mais cuidadosa dos sistemas complexos. Por exemplo, na genética, a localização de características quantitativas em um cromossomo e a interpretação de microarranjos relacionam-se a misturas.\newline
Abordagens Bayesianas à modelagem de misturas têm atraído grande interesse entre pesquisadores e praticantes. O paradigma Bayesiano permite que declarações de probabilidade sejam feitas diretamente sobre os parâmetros desconhecidos e opiniões prévias a serem incluídas na análise e modelagem do modelo. Essa estrutura também permite que a dificuldade de um modelo de mistura seja decomposta em um conjunto de estruturas mais simples, através do uso de variáveis latentes. 

## Mistura finita

A descrição de uma mistura de distribuições é simples: qualquer combinação convexa de outras distribuições $f_i$ é uma mistura, como mostra a combinação abaixo:
\[
\quad \sum_{i=1}^{k} p_{i} f_{i}(x), \quad \sum_{i=1}^{k} p_{i}=1, \quad k>1
\]

Na maioria dos casos, as distribuições $f_i$ são de uma família paramétrica, com parâmetro desconhecido $\theta_i$, levando ao modelo de mistura paramétrica:
\[
\sum_{i=1}^{k} p_{i} f\left(x | \theta_{i}\right)
\]

Além disso, o comportamento da cauda de uma mistura é sempre descrito por um ou dois de seus componentes e que, portanto, deve refletir a escolha da família paramétrica $f\left(.| \theta_{i}\right)$. Note também que a representação de misturas como combinações convexas de distribuições implica na propriedade de cálculo dos momentos: 
$$\mathbb{E}\left[X^{m}\right]=\sum_{i=1}^{k} p_{i} \mathbb{E}^{f_{i}}\left[X^{m}\right]$$

A verossimilhança $\mathbb{L}(\underline{\theta}, \underline{p} | \underline{x})=\prod_{i=1}^{n} \sum_{j=1}^{k} p_{j} f\left(x_{i} | \theta_{j}\right)$ de uma mistura de k distribuições tem $k^n$ termos, o que impossibilita alguma solução analítica.


## Dados faltantes

\[
X_{i}\left|Z_{i}=z \sim f\left(x | \theta_{z}\right), \qquad Z_{i} \sim \mathcal{M}_{k}\left(1 ; p_{1}, \ldots, p_{k}\right)\right.
\]

# Algoritmo


# Simulação

- mistura de normais com apenas as médias desconhecidas
- apresentar os problemas com abordagem EM

Mistura: $p N\left(\mu_{1}, 1\right)+(1-p) N\left(\mu_{2}, 1\right)$, $\left(\mu_{1}, \mu_{2}, p\right)=(0,2.5,0.7)$


```{r}
rmixnorm <- function(n, mean = list(0), p = list(0.5), sd = NULL) {
  if (length(mean) != length(p)) errorCondition("Tamanhos diferem")
  
  sd <- rep(1, length(p))
  norm <- sample(1:length(p), 1, replace = TRUE, p)
  rnorm(n, mean[[norm]], sd[[norm]])
}

rz <- function(n, k, p) {
  sample(1:k, n, replace = TRUE, p)
}

logmixnorm <- function(x, z, mean = list(0), p = list(0.5), sd = NULL) {
  if (length(x) != length(z) || length(mean) != length(p)) errorCondition("Tamanhos diferem")
  
  sd <- rep(1, length(p))
  k <- length(p)
  n <- length(x)
  
  sum(purrr::map2(
    as.vector(x), as.vector(z),
    function(x, z) { log(p[[z]]) + dnorm(x, mean[[z]], 1, log = TRUE) }
    ))
  
  # sum <- 0
  # for (i in 1:n) {
  #   sum <- sum + log(p[[z[i]]]) + dnorm(x[i], mean[[z[i]]], 1, log = TRUE)
  # }
  # sum
}
```

```{r}
n <- 500
mean <- list(0, 2.5)
p <- list(0.7, 0.3)
k <- 2

mixture_simul <- rmixnorm(n, mean, p)

z <- rz(n, k, p)

logmixnorm(mixture_simul, z, mean, p)
```




# Resultado



\bibliographystyle{plain}
\bibliography{bibliography}

